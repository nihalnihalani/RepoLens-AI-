{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPebd3Xkvgrdm31Yq9pwjPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihalnihalani/RepoLens-AI-/blob/main/Github_working_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obJZ8ClWxvxV"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install requests pandas matplotlib networkx ast2json radon pygithub -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import base64\n",
        "import re\n",
        "import ast\n",
        "import networkx as nx\n",
        "import radon.metrics as metrics\n",
        "import radon.complexity as complexity\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, Counter\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import numpy as np\n",
        "from github import Github\n",
        "import time\n",
        "\n",
        "\n",
        "class GitHubRepoInfo:\n",
        "    \"\"\"Enhanced class to get comprehensive information about a GitHub repository.\"\"\"\n",
        "\n",
        "    def __init__(self, token=None):\n",
        "        \"\"\"Initialize with optional GitHub API token.\"\"\"\n",
        "        self.base_url = \"https://api.github.com\"\n",
        "        self.headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
        "        self.token = token\n",
        "\n",
        "        # Set up authentication\n",
        "        if token:\n",
        "            self.headers[\"Authorization\"] = f\"token {token}\"\n",
        "            self.github = Github(token)\n",
        "        elif os.environ.get(\"GITHUB_TOKEN\"):\n",
        "            self.token = os.environ.get(\"GITHUB_TOKEN\")\n",
        "            self.headers[\"Authorization\"] = f\"token {self.token}\"\n",
        "            self.github = Github(self.token)\n",
        "        else:\n",
        "            self.github = Github()\n",
        "\n",
        "        # Configure rate limit handling\n",
        "        self.rate_limit_remaining = 5000\n",
        "        self.rate_limit_reset = datetime.now()\n",
        "\n",
        "    def _check_rate_limit(self):\n",
        "        \"\"\"Check API rate limit and wait if necessary.\"\"\"\n",
        "        if self.rate_limit_remaining <= 10:\n",
        "            reset_time = self.rate_limit_reset\n",
        "            current_time = datetime.now()\n",
        "\n",
        "            if reset_time > current_time:\n",
        "                wait_time = (reset_time - current_time).total_seconds() + 10  # Add buffer\n",
        "                print(f\"Rate limit nearly exhausted. Waiting {wait_time:.0f} seconds for reset.\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "        # Update rate limit info after each API call\n",
        "        response = requests.get(f\"{self.base_url}/rate_limit\", headers=self.headers)\n",
        "        if response.status_code == 200:\n",
        "            rate_data = response.json()\n",
        "            self.rate_limit_remaining = rate_data[\"resources\"][\"core\"][\"remaining\"]\n",
        "            self.rate_limit_reset = datetime.fromtimestamp(rate_data[\"resources\"][\"core\"][\"reset\"])\n",
        "\n",
        "    def _paginated_get(self, url, params=None, max_items=None):\n",
        "        \"\"\"Handle paginated API responses with rate limit awareness.\"\"\"\n",
        "        if params is None:\n",
        "            params = {}\n",
        "\n",
        "        items = []\n",
        "        page = 1\n",
        "        per_page = min(100, params.get(\"per_page\", 30))\n",
        "        params[\"per_page\"] = per_page\n",
        "\n",
        "        while True:\n",
        "            self._check_rate_limit()\n",
        "\n",
        "            params[\"page\"] = page\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                page_items = response.json()\n",
        "                if not page_items:\n",
        "                    break\n",
        "\n",
        "                items.extend(page_items)\n",
        "                page += 1\n",
        "\n",
        "                # Check if we've reached the requested limit\n",
        "                if max_items and len(items) >= max_items:\n",
        "                    return items[:max_items]\n",
        "\n",
        "                # Check if we've reached the end (GitHub returns fewer items than requested)\n",
        "                if len(page_items) < per_page:\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"Error {response.status_code}: {response.text}\")\n",
        "                break\n",
        "\n",
        "        return items\n",
        "\n",
        "    def get_repo_info(self, owner, repo):\n",
        "        \"\"\"Get basic repository information.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Error {response.status_code}: {response.text}\")\n",
        "            return None\n",
        "\n",
        "    def get_contributors(self, owner, repo, max_contributors=None):\n",
        "        \"\"\"Get repository contributors with pagination support.\"\"\"\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/contributors\"\n",
        "        return self._paginated_get(url, max_items=max_contributors)\n",
        "\n",
        "    def get_languages(self, owner, repo):\n",
        "        \"\"\"Get languages used in the repository.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/languages\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Error getting languages: {response.status_code}\")\n",
        "            return {}\n",
        "\n",
        "    def get_commits(self, owner, repo, params=None, max_commits=None):\n",
        "        \"\"\"Get commits with enhanced filtering and pagination.\"\"\"\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/commits\"\n",
        "        return self._paginated_get(url, params=params, max_items=max_commits)\n",
        "\n",
        "    def get_commit_activity(self, owner, repo):\n",
        "        \"\"\"Get commit activity stats for the past year.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/stats/commit_activity\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        elif response.status_code == 202:\n",
        "            # GitHub is computing the statistics, wait and retry\n",
        "            print(\"GitHub is computing statistics, waiting and retrying...\")\n",
        "            time.sleep(2)\n",
        "            return self.get_commit_activity(owner, repo)\n",
        "        else:\n",
        "            print(f\"Error getting commit activity: {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "    def get_code_frequency(self, owner, repo):\n",
        "        \"\"\"Get weekly code addition and deletion statistics.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/stats/code_frequency\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        elif response.status_code == 202:\n",
        "            # GitHub is computing the statistics, wait and retry\n",
        "            print(\"GitHub is computing statistics, waiting and retrying...\")\n",
        "            time.sleep(2)\n",
        "            return self.get_code_frequency(owner, repo)\n",
        "        else:\n",
        "            print(f\"Error getting code frequency: {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "    def get_contributor_activity(self, owner, repo):\n",
        "        \"\"\"Get contributor commit activity over time.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/stats/contributors\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        elif response.status_code == 202:\n",
        "            # GitHub is computing the statistics, wait and retry\n",
        "            print(\"GitHub is computing statistics, waiting and retrying...\")\n",
        "            time.sleep(2)\n",
        "            return self.get_contributor_activity(owner, repo)\n",
        "        else:\n",
        "            print(f\"Error getting contributor activity: {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "    def get_branches(self, owner, repo):\n",
        "        \"\"\"Get repository branches.\"\"\"\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/branches\"\n",
        "        return self._paginated_get(url)\n",
        "\n",
        "    def get_releases(self, owner, repo, max_releases=None):\n",
        "        \"\"\"Get repository releases with pagination support.\"\"\"\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/releases\"\n",
        "        return self._paginated_get(url, max_items=max_releases)\n",
        "\n",
        "    def get_issues(self, owner, repo, state=\"all\", max_issues=None, params=None):\n",
        "        \"\"\"Get repository issues with enhanced filtering.\"\"\"\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/issues\"\n",
        "        if params is None:\n",
        "            params = {}\n",
        "        params[\"state\"] = state\n",
        "        return self._paginated_get(url, params=params, max_items=max_issues)\n",
        "\n",
        "    def get_issue_timeline(self, owner, repo, days_back=180):\n",
        "        \"\"\"Analyze issue creation and closing over time.\"\"\"\n",
        "        # Get issues including closed ones\n",
        "        issues = self.get_issues(owner, repo, state=\"all\")\n",
        "\n",
        "        # Prepare timeline data\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=days_back)\n",
        "\n",
        "        # Initialize daily counters\n",
        "        date_range = pd.date_range(start=start_date, end=end_date)\n",
        "        created_counts = {d.strftime('%Y-%m-%d'): 0 for d in date_range}\n",
        "        closed_counts = {d.strftime('%Y-%m-%d'): 0 for d in date_range}\n",
        "\n",
        "        # Collect issue creation and closing dates\n",
        "        for issue in issues:\n",
        "            created_at = datetime.strptime(issue['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            if created_at >= start_date:\n",
        "                created_counts[created_at.strftime('%Y-%m-%d')] += 1\n",
        "\n",
        "            if issue['state'] == 'closed' and issue.get('closed_at'):\n",
        "                closed_at = datetime.strptime(issue['closed_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                if closed_at >= start_date:\n",
        "                    closed_counts[closed_at.strftime('%Y-%m-%d')] += 1\n",
        "\n",
        "        # Calculate resolution times for closed issues\n",
        "        resolution_times = []\n",
        "        for issue in issues:\n",
        "            if issue['state'] == 'closed' and issue.get('closed_at'):\n",
        "                created_at = datetime.strptime(issue['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                closed_at = datetime.strptime(issue['closed_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                resolution_time = (closed_at - created_at).total_seconds() / 3600  # hours\n",
        "                resolution_times.append(resolution_time)\n",
        "\n",
        "        # Calculate issue labels distribution\n",
        "        label_counts = defaultdict(int)\n",
        "        for issue in issues:\n",
        "            for label in issue.get('labels', []):\n",
        "                label_counts[label['name']] += 1\n",
        "\n",
        "        return {\n",
        "            'created': created_counts,\n",
        "            'closed': closed_counts,\n",
        "            'resolution_times': resolution_times,\n",
        "            'labels': dict(label_counts)\n",
        "        }\n",
        "\n",
        "    def get_pull_requests(self, owner, repo, state=\"all\", max_prs=None, params=None):\n",
        "        \"\"\"Get repository pull requests with enhanced filtering.\"\"\"\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/pulls\"\n",
        "        if params is None:\n",
        "            params = {}\n",
        "        params[\"state\"] = state\n",
        "        return self._paginated_get(url, params=params, max_items=max_prs)\n",
        "\n",
        "    def get_pr_timeline(self, owner, repo, days_back=180):\n",
        "        \"\"\"Analyze PR creation, closing, and metrics over time.\"\"\"\n",
        "        # Get PRs including closed and merged ones\n",
        "        prs = self.get_pull_requests(owner, repo, state=\"all\")\n",
        "\n",
        "        # Prepare timeline data\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=days_back)\n",
        "\n",
        "        # Initialize daily counters\n",
        "        date_range = pd.date_range(start=start_date, end=end_date)\n",
        "        created_counts = {d.strftime('%Y-%m-%d'): 0 for d in date_range}\n",
        "        closed_counts = {d.strftime('%Y-%m-%d'): 0 for d in date_range}\n",
        "        merged_counts = {d.strftime('%Y-%m-%d'): 0 for d in date_range}\n",
        "\n",
        "        # Track metrics\n",
        "        merge_times = []\n",
        "        pr_sizes = []\n",
        "\n",
        "        # Collect PR data\n",
        "        for pr in prs:\n",
        "            created_at = datetime.strptime(pr['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            if created_at >= start_date:\n",
        "                created_counts[created_at.strftime('%Y-%m-%d')] += 1\n",
        "\n",
        "                # Get PR size (additions + deletions)\n",
        "                if pr.get('additions') is not None and pr.get('deletions') is not None:\n",
        "                    pr_sizes.append({\n",
        "                        'additions': pr['additions'],\n",
        "                        'deletions': pr['deletions'],\n",
        "                        'total': pr['additions'] + pr['deletions'],\n",
        "                        'files_changed': pr.get('changed_files', 0)\n",
        "                    })\n",
        "\n",
        "            # Check if PR is closed\n",
        "            if pr['state'] == 'closed':\n",
        "                closed_at = datetime.strptime(pr['closed_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                if closed_at >= start_date:\n",
        "                    closed_counts[closed_at.strftime('%Y-%m-%d')] += 1\n",
        "\n",
        "                    # Check if PR was merged\n",
        "                    if pr['merged_at']:\n",
        "                        merged_at = datetime.strptime(pr['merged_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                        if merged_at >= start_date:\n",
        "                            merged_counts[merged_at.strftime('%Y-%m-%d')] += 1\n",
        "\n",
        "                            # Calculate time to merge\n",
        "                            merge_time = (merged_at - created_at).total_seconds() / 3600  # hours\n",
        "                            merge_times.append(merge_time)\n",
        "\n",
        "        # Calculate acceptance rate\n",
        "        total_closed = sum(closed_counts.values())\n",
        "        total_merged = sum(merged_counts.values())\n",
        "        acceptance_rate = (total_merged / total_closed) * 100 if total_closed > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'created': created_counts,\n",
        "            'closed': closed_counts,\n",
        "            'merged': merged_counts,\n",
        "            'merge_times': merge_times,\n",
        "            'pr_sizes': pr_sizes,\n",
        "            'acceptance_rate': acceptance_rate\n",
        "        }\n",
        "\n",
        "    def get_contents(self, owner, repo, path=\"\", ref=None):\n",
        "        \"\"\"Get repository contents at the specified path.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/contents/{path}\"\n",
        "        params = {}\n",
        "        if ref:\n",
        "            params[\"ref\"] = ref\n",
        "\n",
        "        response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Error getting contents: {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "    def get_readme(self, owner, repo, ref=None):\n",
        "        \"\"\"Get repository README file.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/readme\"\n",
        "        params = {}\n",
        "        if ref:\n",
        "            params[\"ref\"] = ref\n",
        "\n",
        "        response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data.get(\"content\"):\n",
        "                content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n",
        "                return {\n",
        "                    \"name\": data[\"name\"],\n",
        "                    \"path\": data[\"path\"],\n",
        "                    \"content\": content\n",
        "                }\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"README not found or error: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    def get_file_content(self, owner, repo, path, ref=None):\n",
        "        \"\"\"Get the content of a specific file in the repository.\"\"\"\n",
        "        self._check_rate_limit()\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/contents/{path}\"\n",
        "        params = {}\n",
        "        if ref:\n",
        "            params[\"ref\"] = ref\n",
        "\n",
        "        response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data.get(\"content\"):\n",
        "                try:\n",
        "                    content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n",
        "                    return content\n",
        "                except UnicodeDecodeError:\n",
        "                    return \"[Binary file content not displayed]\"\n",
        "            return None\n",
        "        else:\n",
        "            print(f\"Error getting file content: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    def is_text_file(self, file_path):\n",
        "        \"\"\"Determine if a file is likely a text file based on extension.\"\"\"\n",
        "        text_extensions = [\n",
        "            '.txt', '.md', '.rst', '.py', '.js', '.html', '.css', '.java', '.c',\n",
        "            '.cpp', '.h', '.hpp', '.json', '.xml', '.yaml', '.yml', '.toml',\n",
        "            '.ini', '.cfg', '.conf', '.sh', '.bat', '.ps1', '.rb', '.pl', '.php',\n",
        "            '.go', '.rs', '.ts', '.jsx', '.tsx', '.vue', '.swift', '.kt', '.scala',\n",
        "            '.groovy', '.lua', '.r', '.dart', '.ex', '.exs', '.erl', '.hrl',\n",
        "            '.clj', '.hs', '.elm', '.f90', '.f95', '.f03', '.sql', '.gitignore',\n",
        "            '.dockerignore', '.env', '.editorconfig', '.htaccess', '.cs', '.ipynb',\n",
        "            '.R', '.Rmd', '.jl', '.fs', '.ml', '.mli', '.d', '.scm', '.lisp',\n",
        "            '.el', '.m', '.mm', '.vb', '.asm', '.s', '.Dockerfile', '.gradle'\n",
        "        ]\n",
        "\n",
        "        extension = os.path.splitext(file_path)[1].lower()\n",
        "        return extension in text_extensions\n",
        "\n",
        "    def get_recursive_contents(self, owner, repo, path=\"\", max_depth=3, current_depth=0, max_files=1000, ref=None):\n",
        "        \"\"\"Recursively get repository contents with a depth limit and file count limit.\"\"\"\n",
        "        if current_depth >= max_depth:\n",
        "            return []\n",
        "\n",
        "        contents = self.get_contents(owner, repo, path, ref)\n",
        "        results = []\n",
        "        file_count = 0\n",
        "\n",
        "        for item in contents:\n",
        "            if file_count >= max_files:\n",
        "                break\n",
        "\n",
        "            if item[\"type\"] == \"dir\":\n",
        "                # For directories, add the directory itself and recursively get contents\n",
        "                dir_item = {\n",
        "                    \"type\": \"dir\",\n",
        "                    \"name\": item[\"name\"],\n",
        "                    \"path\": item[\"path\"],\n",
        "                    \"contents\": self.get_recursive_contents(\n",
        "                        owner, repo, item[\"path\"], max_depth, current_depth + 1,\n",
        "                        max_files - file_count, ref\n",
        "                    )\n",
        "                }\n",
        "                results.append(dir_item)\n",
        "            else:\n",
        "                # For files, add the file info\n",
        "                results.append({\n",
        "                    \"type\": \"file\",\n",
        "                    \"name\": item[\"name\"],\n",
        "                    \"path\": item[\"path\"],\n",
        "                    \"size\": item[\"size\"],\n",
        "                    \"url\": item[\"html_url\"]\n",
        "                })\n",
        "                file_count += 1\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_all_text_files(self, owner, repo, path=\"\", max_files=50, ref=None):\n",
        "        \"\"\"Get content of all text files in the repository (with limit).\"\"\"\n",
        "        contents = self.get_contents(owner, repo, path, ref)\n",
        "        text_files = []\n",
        "        file_count = 0\n",
        "\n",
        "        # Process current directory\n",
        "        for item in contents:\n",
        "            if file_count >= max_files:\n",
        "                break\n",
        "\n",
        "            if item[\"type\"] == \"file\" and self.is_text_file(item[\"name\"]):\n",
        "                content = self.get_file_content(owner, repo, item[\"path\"], ref)\n",
        "                if content and content != \"[Binary file content not displayed]\":\n",
        "                    text_files.append({\n",
        "                        \"name\": item[\"name\"],\n",
        "                        \"path\": item[\"path\"],\n",
        "                        \"content\": content\n",
        "                    })\n",
        "                    file_count += 1\n",
        "            elif item[\"type\"] == \"dir\":\n",
        "                # Recursively get text files from subdirectories\n",
        "                subdir_files = self.get_all_text_files(\n",
        "                    owner, repo, item[\"path\"], max_files - file_count, ref\n",
        "                )\n",
        "                text_files.extend(subdir_files)\n",
        "                file_count += len(subdir_files)\n",
        "\n",
        "        return text_files\n",
        "\n",
        "    def get_documentation_files(self, owner, repo, ref=None):\n",
        "        \"\"\"Get documentation files from the repository.\"\"\"\n",
        "        # Common documentation file paths and directories\n",
        "        doc_paths = [\n",
        "            \"docs\", \"doc\", \"documentation\", \"wiki\", \"CONTRIBUTING.md\",\n",
        "            \"CONTRIBUTORS.md\", \"CODE_OF_CONDUCT.md\", \"SECURITY.md\",\n",
        "            \"SUPPORT.md\", \"docs/index.md\", \"docs/README.md\", \"docs/getting-started.md\",\n",
        "            \".github/ISSUE_TEMPLATE\", \".github/PULL_REQUEST_TEMPLATE.md\"\n",
        "        ]\n",
        "\n",
        "        doc_files = []\n",
        "\n",
        "        # Try to get each documentation file/directory\n",
        "        for path in doc_paths:\n",
        "            try:\n",
        "                contents = self.get_contents(owner, repo, path, ref)\n",
        "\n",
        "                # If it's a directory, get all markdown files in it\n",
        "                if isinstance(contents, list):\n",
        "                    for item in contents:\n",
        "                        if item[\"type\"] == \"file\" and item[\"name\"].lower().endswith((\".md\", \".rst\", \".txt\")):\n",
        "                            content = self.get_file_content(owner, repo, item[\"path\"], ref)\n",
        "                            if content:\n",
        "                                doc_files.append({\n",
        "                                    \"name\": item[\"name\"],\n",
        "                                    \"path\": item[\"path\"],\n",
        "                                    \"content\": content\n",
        "                                })\n",
        "                # If it's a file, get its content\n",
        "                elif isinstance(contents, dict) and contents.get(\"type\") == \"file\":\n",
        "                    content = self.get_file_content(owner, repo, path, ref)\n",
        "                    if content:\n",
        "                        doc_files.append({\n",
        "                            \"name\": contents[\"name\"],\n",
        "                            \"path\": contents[\"path\"],\n",
        "                            \"content\": content\n",
        "                        })\n",
        "            except:\n",
        "                # Path doesn't exist or access issues\n",
        "                continue\n",
        "\n",
        "        return doc_files\n",
        "\n",
        "    def analyze_ast(self, code, file_path):\n",
        "        \"\"\"Analyze Python code using AST (Abstract Syntax Tree).\"\"\"\n",
        "        if not file_path.endswith('.py'):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            tree = ast.parse(code)\n",
        "\n",
        "            # Extract more detailed information using AST\n",
        "            functions = []\n",
        "            classes = []\n",
        "            imports = []\n",
        "            function_complexities = {}\n",
        "\n",
        "            for node in ast.walk(tree):\n",
        "                # Get function definitions with arguments\n",
        "                if isinstance(node, ast.FunctionDef):\n",
        "                    args = []\n",
        "                    defaults = len(node.args.defaults)\n",
        "                    args_count = len(node.args.args) - defaults\n",
        "\n",
        "                    # Get positional args\n",
        "                    for arg in node.args.args[:args_count]:\n",
        "                        if hasattr(arg, 'arg'):  # Python 3\n",
        "                            args.append(arg.arg)\n",
        "                        else:  # Python 2\n",
        "                            args.append(arg.id)\n",
        "\n",
        "                    # Get args with defaults\n",
        "                    for i, arg in enumerate(node.args.args[args_count:]):\n",
        "                        if hasattr(arg, 'arg'):  # Python 3\n",
        "                            args.append(f\"{arg.arg}=...\")\n",
        "                        else:  # Python 2\n",
        "                            args.append(f\"{arg.id}=...\")\n",
        "\n",
        "                    # Calculate function complexity\n",
        "                    func_complexity = complexity.cc_visit(node)\n",
        "                    function_complexities[node.name] = func_complexity\n",
        "\n",
        "                    # Get docstring if available\n",
        "                    docstring = ast.get_docstring(node)\n",
        "\n",
        "                    functions.append({\n",
        "                        'name': node.name,\n",
        "                        'args': args,\n",
        "                        'complexity': func_complexity,\n",
        "                        'docstring': docstring\n",
        "                    })\n",
        "\n",
        "                # Get class definitions\n",
        "                elif isinstance(node, ast.ClassDef):\n",
        "                    methods = []\n",
        "                    class_docstring = ast.get_docstring(node)\n",
        "\n",
        "                    # Get class methods\n",
        "                    for child in node.body:\n",
        "                        if isinstance(child, ast.FunctionDef):\n",
        "                            method_complexity = complexity.cc_visit(child)\n",
        "                            method_docstring = ast.get_docstring(child)\n",
        "\n",
        "                            methods.append({\n",
        "                                'name': child.name,\n",
        "                                'complexity': method_complexity,\n",
        "                                'docstring': method_docstring\n",
        "                            })\n",
        "\n",
        "                    classes.append({\n",
        "                        'name': node.name,\n",
        "                        'methods': methods,\n",
        "                        'docstring': class_docstring\n",
        "                    })\n",
        "\n",
        "                # Get imports\n",
        "                elif isinstance(node, ast.Import):\n",
        "                    for name in node.names:\n",
        "                        imports.append(name.name)\n",
        "                elif isinstance(node, ast.ImportFrom):\n",
        "                    module = node.module or \"\"\n",
        "                    for name in node.names:\n",
        "                        imports.append(f\"{module}.{name.name}\")\n",
        "\n",
        "            # Calculate overall code complexity\n",
        "            code_complexity = complexity.cc_visit_ast(tree)\n",
        "\n",
        "            # Calculate maintainability index\n",
        "            try:\n",
        "                mi_score = metrics.mi_visit(code, True)\n",
        "            except:\n",
        "                mi_score = None\n",
        "\n",
        "            return {\n",
        "                'functions': functions,\n",
        "                'classes': classes,\n",
        "                'imports': imports,\n",
        "                'complexity': {\n",
        "                    'overall': code_complexity,\n",
        "                    'functions': function_complexities,\n",
        "                    'maintainability_index': mi_score\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except SyntaxError:\n",
        "            print(f\"Syntax error in Python file: {file_path}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing {file_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_js_ts(self, code, file_path):\n",
        "        \"\"\"Analyze JavaScript/TypeScript code using regex with improved patterns.\"\"\"\n",
        "        if not file_path.endswith(('.js', '.ts', '.jsx', '.tsx')):\n",
        "            return None\n",
        "\n",
        "        # More sophisticated regex patterns for JS/TS analysis\n",
        "        results = {\n",
        "            'functions': [],\n",
        "            'classes': [],\n",
        "            'imports': [],\n",
        "            'exports': [],\n",
        "            'hooks': []  # For React hooks\n",
        "        }\n",
        "\n",
        "        # Function patterns (covering various declaration styles)\n",
        "        function_patterns = [\n",
        "            # Regular functions\n",
        "            r'function\\s+(\\w+)\\s*\\(([^)]*)\\)',\n",
        "            # Arrow functions assigned to variables\n",
        "            r'(?:const|let|var)\\s+(\\w+)\\s*=\\s*(?:\\([^)]*\\)|[^=]*)\\s*=>\\s*{',\n",
        "            # Class methods\n",
        "            r'(?:async\\s+)?(\\w+)\\s*\\(([^)]*)\\)\\s*{',\n",
        "            # Object methods\n",
        "            r'(\\w+)\\s*:\\s*function\\s*\\(([^)]*)\\)'\n",
        "        ]\n",
        "\n",
        "        for pattern in function_patterns:\n",
        "            for match in re.finditer(pattern, code):\n",
        "                func_name = match.group(1)\n",
        "                args = match.group(2).strip() if len(match.groups()) > 1 else \"\"\n",
        "                results['functions'].append({\n",
        "                    'name': func_name,\n",
        "                    'args': args\n",
        "                })\n",
        "\n",
        "        # Class pattern\n",
        "        class_pattern = r'class\\s+(\\w+)(?:\\s+extends\\s+(\\w+))?\\s*{([^}]*)}'\n",
        "        for match in re.finditer(class_pattern, code, re.DOTALL):\n",
        "            class_name = match.group(1)\n",
        "            parent_class = match.group(2) if match.group(2) else None\n",
        "            class_body = match.group(3)\n",
        "\n",
        "            # Find methods in class\n",
        "            methods = []\n",
        "            method_pattern = r'(?:async\\s+)?(\\w+)\\s*\\(([^)]*)\\)\\s*{([^}]*)}'\n",
        "            for method_match in re.finditer(method_pattern, class_body):\n",
        "                method_name = method_match.group(1)\n",
        "                methods.append(method_name)\n",
        "\n",
        "            results['classes'].append({\n",
        "                'name': class_name,\n",
        "                'extends': parent_class,\n",
        "                'methods': methods\n",
        "            })\n",
        "\n",
        "        # Import patterns\n",
        "        import_patterns = [\n",
        "            # ES6 imports\n",
        "            r'import\\s+(?:{([^}]*)}|\\*\\s+as\\s+(\\w+)|(\\w+))\\s+from\\s+[\\'\"]([^\\'\"]+)[\\'\"]',\n",
        "            # CommonJS requires\n",
        "            r'(?:const|let|var)\\s+(?:{([^}]*)}|(\\w+))\\s*=\\s*require\\([\\'\"]([^\\'\"]+)[\\'\"]\\)'\n",
        "        ]\n",
        "\n",
        "        for pattern in import_patterns:\n",
        "            for match in re.finditer(pattern, code):\n",
        "                groups = match.groups()\n",
        "                if groups[0]:  # Destructured import\n",
        "                    imports = [name.strip() for name in groups[0].split(',')]\n",
        "                    for imp in imports:\n",
        "                        results['imports'].append(imp)\n",
        "                elif groups[1]:  # Namespace import (import * as X)\n",
        "                    results['imports'].append(groups[1])\n",
        "                elif groups[2]:  # Default import\n",
        "                    results['imports'].append(groups[2])\n",
        "                elif groups[3]:  # Module name\n",
        "                    results['imports'].append(groups[3])\n",
        "\n",
        "        # React hooks detection (for React files)\n",
        "        if file_path.endswith(('.jsx', '.tsx')):\n",
        "            hook_pattern = r'use([A-Z]\\w+)\\s*\\('\n",
        "            for match in re.finditer(hook_pattern, code):\n",
        "                hook_name = 'use' + match.group(1)\n",
        "                results['hooks'].append(hook_name)\n",
        "\n",
        "        # Export patterns\n",
        "        export_patterns = [\n",
        "            # Named exports\n",
        "            r'export\\s+(?:const|let|var|function|class)\\s+(\\w+)',\n",
        "            # Default exports\n",
        "            r'export\\s+default\\s+(?:function|class)?\\s*(\\w+)?'\n",
        "        ]\n",
        "\n",
        "        for pattern in export_patterns:\n",
        "            for match in re.finditer(pattern, code):\n",
        "                if match.group(1):\n",
        "                    results['exports'].append(match.group(1))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_code_summary(self, file_content, file_path):\n",
        "        \"\"\"Extract comprehensive summary information from code files.\"\"\"\n",
        "        extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        # Initialize summary\n",
        "        summary = {\n",
        "            \"functions\": [],\n",
        "            \"classes\": [],\n",
        "            \"imports\": [],\n",
        "            \"description\": \"\",\n",
        "            \"complexity\": None\n",
        "        }\n",
        "\n",
        "        # Extract Python definitions with AST\n",
        "        if extension == '.py':\n",
        "            ast_result = self.analyze_ast(file_content, file_path)\n",
        "            if ast_result:\n",
        "                summary[\"functions\"] = [f[\"name\"] for f in ast_result[\"functions\"]]\n",
        "                summary[\"classes\"] = [c[\"name\"] for c in ast_result[\"classes\"]]\n",
        "                summary[\"imports\"] = ast_result[\"imports\"]\n",
        "                summary[\"complexity\"] = ast_result[\"complexity\"]\n",
        "\n",
        "                # Try to extract module docstring\n",
        "                try:\n",
        "                    tree = ast.parse(file_content)\n",
        "                    module_docstring = ast.get_docstring(tree)\n",
        "                    if module_docstring:\n",
        "                        summary[\"description\"] = module_docstring\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                # Add detailed function and class info\n",
        "                summary[\"detailed_functions\"] = ast_result[\"functions\"]\n",
        "                summary[\"detailed_classes\"] = ast_result[\"classes\"]\n",
        "\n",
        "        # Extract JavaScript/TypeScript definitions\n",
        "        elif extension in ['.js', '.ts', '.jsx', '.tsx']:\n",
        "            js_result = self.analyze_js_ts(file_content, file_path)\n",
        "            if js_result:\n",
        "                summary[\"functions\"] = [f[\"name\"] for f in js_result[\"functions\"]]\n",
        "                summary[\"classes\"] = [c[\"name\"] for c in js_result[\"classes\"]]\n",
        "                summary[\"imports\"] = js_result[\"imports\"]\n",
        "\n",
        "                # Add detailed function and class info\n",
        "                summary[\"detailed_functions\"] = js_result[\"functions\"]\n",
        "                summary[\"detailed_classes\"] = js_result[\"classes\"]\n",
        "                summary[\"hooks\"] = js_result.get(\"hooks\", [])\n",
        "                summary[\"exports\"] = js_result.get(\"exports\", [])\n",
        "\n",
        "        # Calculate basic code metrics for any text file\n",
        "        if file_content:\n",
        "            lines = file_content.split('\\n')\n",
        "            code_lines = 0\n",
        "            comment_lines = 0\n",
        "            blank_lines = 0\n",
        "\n",
        "            comment_prefixes = ['#', '//', '/*', '*', '<!--']\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    blank_lines += 1\n",
        "                elif any(line.startswith(prefix) for prefix in comment_prefixes):\n",
        "                    comment_lines += 1\n",
        "                else:\n",
        "                    code_lines += 1\n",
        "\n",
        "            summary[\"metrics\"] = {\n",
        "                \"total_lines\": len(lines),\n",
        "                \"code_lines\": code_lines,\n",
        "                \"comment_lines\": comment_lines,\n",
        "                \"blank_lines\": blank_lines,\n",
        "                \"comment_ratio\": comment_lines / max(1, code_lines + comment_lines)\n",
        "            }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def analyze_dependencies(self, owner, repo, max_files=100):\n",
        "        \"\"\"Analyze code dependencies across the repository.\"\"\"\n",
        "        # Get Python and JavaScript files\n",
        "        text_files = self.get_all_text_files(owner, repo, max_files=max_files)\n",
        "\n",
        "        # Filter for Python and JS/TS files\n",
        "        code_files = [f for f in text_files if f[\"name\"].endswith(('.py', '.js', '.ts', '.jsx', '.tsx'))]\n",
        "\n",
        "        # Track dependencies\n",
        "        dependencies = {\n",
        "            'internal': defaultdict(set),  # File to file dependencies\n",
        "            'external': defaultdict(set),  # External package dependencies by file\n",
        "            'modules': defaultdict(set)    # Defined modules/components by file\n",
        "        }\n",
        "\n",
        "        # Extract module names from file paths\n",
        "        file_to_module = {}\n",
        "        for file in code_files:\n",
        "            # Convert file path to potential module name\n",
        "            module_path = os.path.splitext(file[\"path\"])[0].replace('/', '.')\n",
        "            file_to_module[file[\"path\"]] = module_path\n",
        "\n",
        "            # Track what each file defines\n",
        "            summary = self.extract_code_summary(file[\"content\"], file[\"path\"])\n",
        "\n",
        "            if file[\"name\"].endswith('.py'):\n",
        "                for function in summary.get(\"functions\", []):\n",
        "                    dependencies['modules'][file[\"path\"]].add(f\"{module_path}.{function}\")\n",
        "                for class_name in summary.get(\"classes\", []):\n",
        "                    dependencies['modules'][file[\"path\"]].add(f\"{module_path}.{class_name}\")\n",
        "            else:  # JS/TS files\n",
        "                for export in summary.get(\"exports\", []):\n",
        "                    dependencies['modules'][file[\"path\"]].add(export)\n",
        "\n",
        "        # Analyze imports/dependencies\n",
        "        for file in code_files:\n",
        "            summary = self.extract_code_summary(file[\"content\"], file[\"path\"])\n",
        "\n",
        "            for imp in summary.get(\"imports\", []):\n",
        "                # Check if this is an internal import\n",
        "                is_internal = False\n",
        "\n",
        "                if file[\"name\"].endswith('.py'):\n",
        "                    # For Python, check if the import matches any module path\n",
        "                    for module_path in file_to_module.values():\n",
        "                        if imp == module_path or imp.startswith(f\"{module_path}.\"):\n",
        "                            is_internal = True\n",
        "                            # Find the file that defines this module\n",
        "                            for f_path, m_path in file_to_module.items():\n",
        "                                if m_path == imp.split('.')[0]:\n",
        "                                    dependencies['internal'][file[\"path\"]].add(f_path)\n",
        "                                    break\n",
        "                else:\n",
        "                    # For JS/TS, check relative imports\n",
        "                    if imp.startswith('./') or imp.startswith('../'):\n",
        "                        is_internal = True\n",
        "                        # Try to resolve the relative import\n",
        "                        src_dir = os.path.dirname(file[\"path\"])\n",
        "                        target_path = os.path.normpath(os.path.join(src_dir, imp))\n",
        "\n",
        "                        # Add known extensions if not specified\n",
        "                        if '.' not in os.path.basename(target_path):\n",
        "                            for ext in ['.js', '.ts', '.jsx', '.tsx']:\n",
        "                                test_path = f\"{target_path}{ext}\"\n",
        "                                if test_path in file_to_module:\n",
        "                                    dependencies['internal'][file[\"path\"]].add(test_path)\n",
        "                                    break\n",
        "\n",
        "                # If not internal, consider it external\n",
        "                if not is_internal:\n",
        "                    # Clean up the import name (remove relative path parts)\n",
        "                    if not file[\"name\"].endswith('.py'):\n",
        "                        imp = imp.split('/')[0]  # Take the package name part\n",
        "                    dependencies['external'][file[\"path\"]].add(imp)\n",
        "\n",
        "        return dependencies\n",
        "\n",
        "    def create_dependency_graph(self, dependencies):\n",
        "        \"\"\"Create a NetworkX graph from dependencies for visualization.\"\"\"\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add nodes for files\n",
        "        for file_path in dependencies['internal'].keys():\n",
        "            G.add_node(file_path, type='file')\n",
        "\n",
        "        # Add edges for internal dependencies\n",
        "        for file_path, deps in dependencies['internal'].items():\n",
        "            for dep in deps:\n",
        "                G.add_edge(file_path, dep)\n",
        "\n",
        "        # Add nodes and edges for external dependencies\n",
        "        external_nodes = set()\n",
        "        for file_path, deps in dependencies['external'].items():\n",
        "            for dep in deps:\n",
        "                external_node = f\"ext:{dep}\"\n",
        "                if external_node not in external_nodes:\n",
        "                    G.add_node(external_node, type='external')\n",
        "                    external_nodes.add(external_node)\n",
        "                G.add_edge(file_path, external_node)\n",
        "\n",
        "        return G\n",
        "\n",
        "    def get_repo_text_summary(self, owner, repo, max_files=25):\n",
        "        \"\"\"Extract and summarize text content from the repository with improved metrics.\"\"\"\n",
        "        # Get README\n",
        "        readme = self.get_readme(owner, repo)\n",
        "\n",
        "        # Get documentation\n",
        "        docs = self.get_documentation_files(owner, repo)\n",
        "\n",
        "        # Get key code files (limit to avoid API rate limits)\n",
        "        text_files = self.get_all_text_files(owner, repo, max_files=max_files)\n",
        "\n",
        "        # Analyze code files\n",
        "        code_summary = {}\n",
        "        complexity_metrics = {\n",
        "            'cyclomatic_complexity': [],\n",
        "            'maintainability_index': [],\n",
        "            'comment_ratios': []\n",
        "        }\n",
        "\n",
        "        for file in text_files:\n",
        "            ext = os.path.splitext(file[\"name\"])[1].lower()\n",
        "            if ext in ['.py', '.js', '.ts', '.jsx', '.tsx']:\n",
        "                file_summary = self.extract_code_summary(file[\"content\"], file[\"path\"])\n",
        "                code_summary[file[\"path\"]] = file_summary\n",
        "\n",
        "                # Collect complexity metrics\n",
        "                if file_summary.get('complexity'):\n",
        "                    cc = file_summary['complexity'].get('overall')\n",
        "                    if cc is not None:\n",
        "                        complexity_metrics['cyclomatic_complexity'].append((file[\"path\"], cc))\n",
        "\n",
        "                    mi = file_summary['complexity'].get('maintainability_index')\n",
        "                    if mi is not None:\n",
        "                        complexity_metrics['maintainability_index'].append((file[\"path\"], mi))\n",
        "\n",
        "                if file_summary.get('metrics'):\n",
        "                    comment_ratio = file_summary['metrics'].get('comment_ratio', 0)\n",
        "                    complexity_metrics['comment_ratios'].append((file[\"path\"], comment_ratio))\n",
        "\n",
        "        # Analyze dependencies\n",
        "        dependencies = self.analyze_dependencies(owner, repo, max_files=max_files)\n",
        "\n",
        "        # Summarize repository content by file type\n",
        "        file_types = defaultdict(int)\n",
        "        for file in text_files:\n",
        "            ext = os.path.splitext(file[\"name\"])[1].lower()\n",
        "            file_types[ext] += 1\n",
        "\n",
        "        # Calculate aggregate code metrics\n",
        "        total_code_lines = sum(summary.get('metrics', {}).get('code_lines', 0)\n",
        "                              for summary in code_summary.values())\n",
        "        total_comment_lines = sum(summary.get('metrics', {}).get('comment_lines', 0)\n",
        "                                 for summary in code_summary.values())\n",
        "\n",
        "        aggregate_metrics = {\n",
        "            'total_files': len(text_files),\n",
        "            'total_code_lines': total_code_lines,\n",
        "            'total_comment_lines': total_comment_lines,\n",
        "            'average_comment_ratio': (total_comment_lines / total_code_lines) if total_code_lines > 0 else 0\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"readme\": readme,\n",
        "            \"documentation\": docs,\n",
        "            \"code_summary\": code_summary,\n",
        "            \"complexity_metrics\": complexity_metrics,\n",
        "            \"dependencies\": dependencies,\n",
        "            \"file_type_counts\": dict(file_types),\n",
        "            \"aggregate_metrics\": aggregate_metrics,\n",
        "            \"text_files\": text_files  # Include the actual text file contents\n",
        "        }\n",
        "\n",
        "    def get_temporal_analysis(self, owner, repo):\n",
        "        \"\"\"Perform temporal analysis of repository activity.\"\"\"\n",
        "        # Get commit activity over time\n",
        "        commit_activity = self.get_commit_activity(owner, repo)\n",
        "\n",
        "        # Get code frequency (additions/deletions over time)\n",
        "        code_frequency = self.get_code_frequency(owner, repo)\n",
        "\n",
        "        # Get contributor activity\n",
        "        contributor_activity = self.get_contributor_activity(owner, repo)\n",
        "\n",
        "        # Get issue and PR timelines\n",
        "        issue_timeline = self.get_issue_timeline(owner, repo)\n",
        "        pr_timeline = self.get_pr_timeline(owner, repo)\n",
        "\n",
        "        # Process data for visualization\n",
        "        # - Weekly commit counts\n",
        "        weekly_commits = []\n",
        "        if commit_activity:\n",
        "            for week in commit_activity:\n",
        "                date = datetime.fromtimestamp(week['week'])\n",
        "                weekly_commits.append({\n",
        "                    'date': date.strftime('%Y-%m-%d'),\n",
        "                    'total': week['total'],\n",
        "                    'days': week['days']  # Daily breakdown within the week\n",
        "                })\n",
        "\n",
        "        # - Weekly code changes\n",
        "        weekly_code_changes = []\n",
        "        if code_frequency:\n",
        "            for item in code_frequency:\n",
        "                date = datetime.fromtimestamp(item[0])\n",
        "                weekly_code_changes.append({\n",
        "                    'date': date.strftime('%Y-%m-%d'),\n",
        "                    'additions': item[1],\n",
        "                    'deletions': -item[2]  # Convert to positive for visualization\n",
        "                })\n",
        "\n",
        "        # - Contributor timeline\n",
        "        contributor_timeline = {}\n",
        "        if contributor_activity:\n",
        "            for contributor in contributor_activity:\n",
        "                author = contributor['author']['login']\n",
        "                weeks = contributor['weeks']\n",
        "\n",
        "                if author not in contributor_timeline:\n",
        "                    contributor_timeline[author] = []\n",
        "\n",
        "                for week in weeks:\n",
        "                    if week['c'] > 0:  # Only include weeks with commits\n",
        "                        date = datetime.fromtimestamp(week['w'])\n",
        "                        contributor_timeline[author].append({\n",
        "                            'date': date.strftime('%Y-%m-%d'),\n",
        "                            'commits': week['c'],\n",
        "                            'additions': week['a'],\n",
        "                            'deletions': week['d']\n",
        "                        })\n",
        "\n",
        "        return {\n",
        "            'weekly_commits': weekly_commits,\n",
        "            'weekly_code_changes': weekly_code_changes,\n",
        "            'contributor_timeline': contributor_timeline,\n",
        "            'issue_timeline': issue_timeline,\n",
        "            'pr_timeline': pr_timeline\n",
        "        }\n",
        "\n",
        "    def get_all_info(self, owner, repo):\n",
        "        \"\"\"Get comprehensive information about a repository with enhanced metrics.\"\"\"\n",
        "        result = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"basic_info\": self.get_repo_info(owner, repo)\n",
        "        }\n",
        "\n",
        "        if not result[\"basic_info\"]:\n",
        "            print(f\"Could not retrieve repository information for {owner}/{repo}\")\n",
        "            return None\n",
        "\n",
        "        print(\"Getting repository statistics...\")\n",
        "\n",
        "        # Get additional information\n",
        "        result[\"languages\"] = self.get_languages(owner, repo)\n",
        "        result[\"contributors\"] = self.get_contributors(owner, repo, max_contributors=30)\n",
        "        result[\"recent_commits\"] = self.get_commits(owner, repo, max_commits=30)\n",
        "        result[\"branches\"] = self.get_branches(owner, repo)\n",
        "        result[\"releases\"] = self.get_releases(owner, repo, max_releases=10)\n",
        "        result[\"open_issues\"] = self.get_issues(owner, repo, state=\"open\", max_issues=50)\n",
        "        result[\"open_pull_requests\"] = self.get_pull_requests(owner, repo, state=\"open\", max_prs=50)\n",
        "        result[\"root_contents\"] = self.get_contents(owner, repo)\n",
        "\n",
        "        print(\"Analyzing repository content...\")\n",
        "\n",
        "        # Get text content and documentation\n",
        "        result[\"text_content\"] = self.get_repo_text_summary(owner, repo, max_files=30)\n",
        "\n",
        "        print(\"Analyzing repository activity over time...\")\n",
        "\n",
        "        # Get temporal analysis\n",
        "        result[\"temporal_analysis\"] = self.get_temporal_analysis(owner, repo)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def display_repo_info(self, repo_data):\n",
        "        \"\"\"Display repository information in a Colab-friendly format with enhanced visualizations.\"\"\"\n",
        "        if not repo_data or not repo_data[\"basic_info\"]:\n",
        "            return\n",
        "\n",
        "        basic = repo_data[\"basic_info\"]\n",
        "\n",
        "        # Display basic repository information\n",
        "        display(HTML(f\"\"\"\n",
        "        <h1 style=\"text-align:center;\">Repository: {basic['full_name']}</h1>\n",
        "        <div style=\"text-align:center;\"><img src=\"{basic.get('owner', {}).get('avatar_url', '')}\" width=\"100\" height=\"100\" style=\"border-radius:50%\"></div>\n",
        "        <div style=\"background-color:#f5f5f5; padding:15px; border-radius:5px; margin:10px 0;\">\n",
        "            <p><strong>Description:</strong> {basic['description'] or 'No description'}</p>\n",
        "            <p><strong>URL:</strong> <a href=\"{basic['html_url']}\" target=\"_blank\">{basic['html_url']}</a></p>\n",
        "            <p><strong>Created:</strong> {basic['created_at']}</p>\n",
        "            <p><strong>Last updated:</strong> {basic['updated_at']}</p>\n",
        "            <p><strong>Default branch:</strong> {basic['default_branch']}</p>\n",
        "            <p><strong>Stars:</strong> {basic['stargazers_count']}</p>\n",
        "            <p><strong>Forks:</strong> {basic['forks_count']}</p>\n",
        "            <p><strong>Open issues:</strong> {basic['open_issues_count']}</p>\n",
        "            <p><strong>License:</strong> {basic['license']['name'] if basic.get('license') else 'Not specified'}</p>\n",
        "            <p><strong>Topics:</strong> {', '.join(basic.get('topics', ['None']))}</p>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "        # Display language distribution\n",
        "        if repo_data[\"languages\"]:\n",
        "            display(Markdown(\"## Languages\"))\n",
        "\n",
        "            # Create DataFrame for languages\n",
        "            lang_data = []\n",
        "            total = sum(repo_data[\"languages\"].values())\n",
        "            for lang, bytes_count in repo_data[\"languages\"].items():\n",
        "                percentage = (bytes_count / total) * 100\n",
        "                lang_data.append({\n",
        "                    \"Language\": lang,\n",
        "                    \"Bytes\": bytes_count,\n",
        "                    \"Percentage\": percentage\n",
        "                })\n",
        "\n",
        "            lang_df = pd.DataFrame(lang_data)\n",
        "            display(lang_df)\n",
        "\n",
        "            # Create pie chart\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.pie(lang_df[\"Percentage\"], labels=lang_df[\"Language\"], autopct='%1.1f%%')\n",
        "            plt.title(\"Language Distribution\")\n",
        "            plt.axis('equal')\n",
        "            plt.show()\n",
        "\n",
        "        # Display contributors\n",
        "        if repo_data[\"contributors\"]:\n",
        "            display(Markdown(\"## Top Contributors\"))\n",
        "\n",
        "            # Create DataFrame for contributors\n",
        "            contrib_data = []\n",
        "            for contributor in repo_data[\"contributors\"][:15]:\n",
        "                contrib_data.append({\n",
        "                    \"Username\": contributor['login'],\n",
        "                    \"Contributions\": contributor['contributions'],\n",
        "                    \"Profile\": contributor['html_url']\n",
        "                })\n",
        "\n",
        "            contrib_df = pd.DataFrame(contrib_data)\n",
        "            display(contrib_df)\n",
        "\n",
        "            # Create bar chart\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.bar(contrib_df[\"Username\"], contrib_df[\"Contributions\"])\n",
        "            plt.title(\"Top Contributors\")\n",
        "            plt.xlabel(\"Contributor\")\n",
        "            plt.ylabel(\"Number of Contributions\")\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Display recent commits\n",
        "        if repo_data[\"recent_commits\"]:\n",
        "            display(Markdown(\"## Recent Commits\"))\n",
        "\n",
        "            commit_data = []\n",
        "            for commit in repo_data[\"recent_commits\"][:10]:\n",
        "                author = commit['commit']['author']['name']\n",
        "                message = commit['commit']['message'].split('\\n')[0]  # First line only\n",
        "                date = commit['commit']['author']['date']\n",
        "                commit_data.append({\n",
        "                    \"Author\": author,\n",
        "                    \"Date\": date,\n",
        "                    \"Message\": message,\n",
        "                    \"URL\": commit.get('html_url', '')\n",
        "                })\n",
        "\n",
        "            commit_df = pd.DataFrame(commit_data)\n",
        "            display(commit_df)\n",
        "\n",
        "        # Display repository structure\n",
        "        if repo_data[\"root_contents\"]:\n",
        "            display(Markdown(\"## Repository Structure\"))\n",
        "\n",
        "            dir_content = []\n",
        "            for item in repo_data[\"root_contents\"]:\n",
        "                dir_content.append({\n",
        "                    \"Name\": item[\"name\"],\n",
        "                    \"Type\": item[\"type\"],\n",
        "                    \"Size\": item.get(\"size\", \"\"),\n",
        "                    \"URL\": item.get(\"html_url\", \"\")\n",
        "                })\n",
        "\n",
        "            dir_df = pd.DataFrame(dir_content)\n",
        "            display(dir_df)\n",
        "\n",
        "        # Display README preview if available\n",
        "        if repo_data[\"text_content\"][\"readme\"]:\n",
        "            display(Markdown(\"## README Preview\"))\n",
        "\n",
        "            readme = repo_data[\"text_content\"][\"readme\"]\n",
        "            display(Markdown(f\"**{readme['name']}**\"))\n",
        "\n",
        "            # Show a preview of the README content (first few lines)\n",
        "            lines = readme[\"content\"].split(\"\\n\")\n",
        "            preview_lines = lines[:min(15, len(lines))]\n",
        "            preview = \"\\n\".join(preview_lines)\n",
        "\n",
        "            display(Markdown(preview))\n",
        "            if len(lines) > 15:\n",
        "                display(Markdown(\"*... (content truncated)* ...\"))\n",
        "\n",
        "        # Display code summary\n",
        "        if repo_data[\"text_content\"][\"code_summary\"]:\n",
        "            display(Markdown(\"## Code Summary\"))\n",
        "\n",
        "            # Count total functions and classes\n",
        "            total_functions = sum(len(summary.get(\"functions\", [])) for summary in repo_data[\"text_content\"][\"code_summary\"].values())\n",
        "            total_classes = sum(len(summary.get(\"classes\", [])) for summary in repo_data[\"text_content\"][\"code_summary\"].values())\n",
        "\n",
        "            # Get aggregate metrics\n",
        "            agg_metrics = repo_data[\"text_content\"][\"aggregate_metrics\"]\n",
        "\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"background-color:#e8f4f8; padding:15px; border-radius:5px; margin:10px 0;\">\n",
        "                <p><strong>Total Files Analyzed:</strong> {agg_metrics['total_files']}</p>\n",
        "                <p><strong>Total Code Lines:</strong> {agg_metrics['total_code_lines']}</p>\n",
        "                <p><strong>Total Comment Lines:</strong> {agg_metrics['total_comment_lines']}</p>\n",
        "                <p><strong>Comment Ratio:</strong> {agg_metrics['average_comment_ratio']:.2f}</p>\n",
        "                <p><strong>Total Functions:</strong> {total_functions}</p>\n",
        "                <p><strong>Total Classes:</strong> {total_classes}</p>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "\n",
        "            # Display complexity metrics\n",
        "            if repo_data[\"text_content\"][\"complexity_metrics\"][\"cyclomatic_complexity\"]:\n",
        "                display(Markdown(\"### Code Complexity\"))\n",
        "\n",
        "                # Get top 10 most complex files\n",
        "                complexity_data = repo_data[\"text_content\"][\"complexity_metrics\"][\"cyclomatic_complexity\"]\n",
        "                complexity_data.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                complex_files = []\n",
        "                for path, cc in complexity_data[:10]:\n",
        "                    complex_files.append({\n",
        "                        \"File\": os.path.basename(path),\n",
        "                        \"Path\": path,\n",
        "                        \"Cyclomatic Complexity\": cc\n",
        "                    })\n",
        "\n",
        "                complex_df = pd.DataFrame(complex_files)\n",
        "                display(complex_df)\n",
        "\n",
        "                # Plot complexity distribution - ensure we have numeric values only\n",
        "                cc_values = []\n",
        "                for _, cc in complexity_data:\n",
        "                    try:\n",
        "                        # Handle both direct numbers and lists that might contain complexity values\n",
        "                        if isinstance(cc, (int, float)):\n",
        "                            cc_values.append(float(cc))\n",
        "                        elif isinstance(cc, list) and len(cc) > 0:\n",
        "                            # If it's a list, use the first numeric value\n",
        "                            for val in cc:\n",
        "                                if isinstance(val, (int, float)):\n",
        "                                    cc_values.append(float(val))\n",
        "                                    break\n",
        "                    except (ValueError, TypeError):\n",
        "                        # Skip values that can't be converted to float\n",
        "                        continue\n",
        "                if cc_values:  # Only plot if we have data\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.hist(cc_values, bins=10, alpha=0.7)\n",
        "                    plt.title(\"Cyclomatic Complexity Distribution\")\n",
        "                    plt.xlabel(\"Complexity\")\n",
        "                    plt.ylabel(\"Number of Files\")\n",
        "                    plt.axvline(np.mean(cc_values), color='r', linestyle='dashed', linewidth=1, label=f\"Mean: {np.mean(cc_values):.2f}\")\n",
        "                    plt.legend()\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "            # Display maintainability index if available\n",
        "            if repo_data[\"text_content\"][\"complexity_metrics\"][\"maintainability_index\"]:\n",
        "                mi_data = repo_data[\"text_content\"][\"complexity_metrics\"][\"maintainability_index\"]\n",
        "                # Ensure we have numeric values only\n",
        "                mi_values = [float(mi) for _, mi in mi_data if mi is not None]\n",
        "\n",
        "                if mi_values:  # Only plot if we have data\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.hist(mi_values, bins=10, alpha=0.7)\n",
        "                    plt.title(\"Maintainability Index Distribution\")\n",
        "                    plt.xlabel(\"Maintainability Index (higher is better)\")\n",
        "                    plt.ylabel(\"Number of Files\")\n",
        "                    plt.axvline(np.mean(mi_values), color='g', linestyle='dashed', linewidth=1, label=f\"Mean: {np.mean(mi_values):.2f}\")\n",
        "                    plt.legend()\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "            # Display file type distribution\n",
        "            if repo_data[\"text_content\"][\"file_type_counts\"]:\n",
        "                display(Markdown(\"### File Type Distribution\"))\n",
        "\n",
        "                file_type_data = []\n",
        "                for ext, count in repo_data[\"text_content\"][\"file_type_counts\"].items():\n",
        "                    if ext:  # Skip empty extensions\n",
        "                        file_type_data.append({\n",
        "                            \"Extension\": ext,\n",
        "                            \"Count\": count\n",
        "                        })\n",
        "\n",
        "                file_type_df = pd.DataFrame(file_type_data)\n",
        "                display(file_type_df)\n",
        "\n",
        "                # Create bar chart\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.bar(file_type_df[\"Extension\"], file_type_df[\"Count\"])\n",
        "                plt.title(\"File Type Distribution\")\n",
        "                plt.xlabel(\"File Extension\")\n",
        "                plt.ylabel(\"Count\")\n",
        "                plt.xticks(rotation=45, ha='right')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        # Display dependency graph if available\n",
        "        if repo_data[\"text_content\"][\"dependencies\"]:\n",
        "            display(Markdown(\"## Code Dependencies\"))\n",
        "\n",
        "            # Create dependency graph\n",
        "            G = self.create_dependency_graph(repo_data[\"text_content\"][\"dependencies\"])\n",
        "\n",
        "            # Display dependency statistics\n",
        "            internal_deps = repo_data[\"text_content\"][\"dependencies\"][\"internal\"]\n",
        "            external_deps = repo_data[\"text_content\"][\"dependencies\"][\"external\"]\n",
        "\n",
        "            # Count unique external dependencies\n",
        "            all_external = set()\n",
        "            for deps in external_deps.values():\n",
        "                all_external.update(deps)\n",
        "\n",
        "            # Find most imported packages\n",
        "            ext_counts = Counter()\n",
        "            for deps in external_deps.values():\n",
        "                ext_counts.update(deps)\n",
        "\n",
        "            top_imports = ext_counts.most_common(10)\n",
        "\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"background-color:#e8f4f8; padding:15px; border-radius:5px; margin:10px 0;\">\n",
        "                <p><strong>Files with Dependencies:</strong> {len(internal_deps) + len(external_deps)}</p>\n",
        "                <p><strong>Internal Dependency Relationships:</strong> {sum(len(deps) for deps in internal_deps.values())}</p>\n",
        "                <p><strong>Unique External Dependencies:</strong> {len(all_external)}</p>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "\n",
        "            # Display most imported packages\n",
        "            if top_imports:\n",
        "                display(Markdown(\"### Most Used External Dependencies\"))\n",
        "\n",
        "                imports_data = []\n",
        "                for pkg, count in top_imports:\n",
        "                    imports_data.append({\n",
        "                        \"Package\": pkg,\n",
        "                        \"Used in # Files\": count\n",
        "                    })\n",
        "\n",
        "                imports_df = pd.DataFrame(imports_data)\n",
        "                display(imports_df)\n",
        "\n",
        "            # Visualize dependency network (if not too large)\n",
        "            if len(G.nodes) <= 50:  # Only visualize if not too complex\n",
        "                try:\n",
        "                    display(Markdown(\"### Dependency Network\"))\n",
        "\n",
        "                    plt.figure(figsize=(12, 12))\n",
        "\n",
        "                    # Node colors based on type\n",
        "                    node_colors = []\n",
        "                    for node in G.nodes:\n",
        "                        if G.nodes[node].get('type') == 'external':\n",
        "                            node_colors.append('red')\n",
        "                        else:\n",
        "                            node_colors.append('skyblue')\n",
        "\n",
        "                    # Node sizes based on connections\n",
        "                    node_sizes = [100 + 50 * G.degree(node) for node in G.nodes]\n",
        "\n",
        "                    # Layout for the graph\n",
        "                    pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)  # Adding seed for reproducibility\n",
        "\n",
        "                    # Draw the graph\n",
        "                    nx.draw_networkx(\n",
        "                        G, pos,\n",
        "                        with_labels=False,\n",
        "                        node_color=node_colors,\n",
        "                        node_size=node_sizes,\n",
        "                        alpha=0.7,\n",
        "                        arrows=True,\n",
        "                        arrowsize=10,\n",
        "                        width=0.5\n",
        "                    )\n",
        "\n",
        "                    # Add labels for external dependencies\n",
        "                    external_labels = {node: node.replace('ext:', '')\n",
        "                                    for node in G.nodes\n",
        "                                    if G.nodes[node].get('type') == 'external'}\n",
        "\n",
        "                    nx.draw_networkx_labels(\n",
        "                        G, pos,\n",
        "                        labels=external_labels,\n",
        "                        font_size=8,\n",
        "                        font_color='black'\n",
        "                    )\n",
        "\n",
        "                    plt.title(\"Code Dependency Network (red=external)\")\n",
        "                    plt.axis('off')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating dependency network visualization: {str(e)}\")\n",
        "                    print(\"Skipping network visualization due to data compatibility issues.\")\n",
        "\n",
        "        # Display temporal analysis\n",
        "        if repo_data[\"temporal_analysis\"][\"weekly_commits\"]:\n",
        "            display(Markdown(\"## Repository Activity Over Time\"))\n",
        "\n",
        "            # Commit activity over time\n",
        "            weekly_commits = repo_data[\"temporal_analysis\"][\"weekly_commits\"]\n",
        "            if weekly_commits:\n",
        "                display(Markdown(\"### Weekly Commit Activity\"))\n",
        "\n",
        "                # Convert to DataFrame for plotting\n",
        "                dates = [datetime.strptime(week['date'], '%Y-%m-%d') for week in weekly_commits]\n",
        "                commits = [week['total'] for week in weekly_commits]\n",
        "\n",
        "                try:\n",
        "                    plt.figure(figsize=(14, 6))\n",
        "                    plt.plot(dates, commits, marker='o', linestyle='-', alpha=0.7)\n",
        "                    plt.title(\"Weekly Commit Activity\")\n",
        "                    plt.xlabel(\"Date\")\n",
        "                    plt.ylabel(\"Number of Commits\")\n",
        "                    plt.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Format x-axis to show dates nicely\n",
        "                    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "                    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
        "                    plt.gcf().autofmt_xdate()\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating commit activity chart: {str(e)}\")\n",
        "                    print(\"Displaying raw data instead:\")\n",
        "                    activity_df = pd.DataFrame({\n",
        "                        'Date': [week['date'] for week in weekly_commits],\n",
        "                        'Commits': [week['total'] for week in weekly_commits]\n",
        "                    })\n",
        "                    display(activity_df.head(10))\n",
        "\n",
        "            # Code changes over time\n",
        "            weekly_code_changes = repo_data[\"temporal_analysis\"][\"weekly_code_changes\"]\n",
        "            if weekly_code_changes:\n",
        "                display(Markdown(\"### Weekly Code Changes\"))\n",
        "\n",
        "                # Convert to DataFrame for plotting\n",
        "                dates = [datetime.strptime(week['date'], '%Y-%m-%d') for week in weekly_code_changes]\n",
        "                additions = [week['additions'] for week in weekly_code_changes]\n",
        "                deletions = [week['deletions'] for week in weekly_code_changes]\n",
        "\n",
        "                try:\n",
        "                    # Convert data to proper format for plotting\n",
        "                    plot_dates = np.array(dates)\n",
        "                    plot_additions = np.array([float(a) for a in additions])\n",
        "                    plot_deletions = np.array([float(d) for d in deletions])\n",
        "\n",
        "                    plt.figure(figsize=(14, 6))\n",
        "                    plt.bar(plot_dates, plot_additions, color='green', alpha=0.6, label='Additions')\n",
        "                    plt.bar(plot_dates, plot_deletions, color='red', alpha=0.6, label='Deletions')\n",
        "                    plt.title(\"Weekly Code Changes\")\n",
        "                    plt.xlabel(\"Date\")\n",
        "                    plt.ylabel(\"Lines Changed\")\n",
        "                    plt.legend()\n",
        "\n",
        "                    # Format x-axis to show dates nicely\n",
        "                    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "                    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
        "                    plt.gcf().autofmt_xdate()\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating code changes chart: {str(e)}\")\n",
        "                    print(\"Displaying raw data instead:\")\n",
        "                    changes_df = pd.DataFrame({\n",
        "                        'Date': [week['date'] for week in weekly_code_changes],\n",
        "                        'Additions': [week['additions'] for week in weekly_code_changes],\n",
        "                        'Deletions': [week['deletions'] for week in weekly_code_changes]\n",
        "                    })\n",
        "                    display(changes_df.head(10))\n",
        "\n",
        "            # Display issue resolution metrics\n",
        "            issue_timeline = repo_data[\"temporal_analysis\"][\"issue_timeline\"]\n",
        "            if issue_timeline and issue_timeline.get('resolution_times'):\n",
        "                display(Markdown(\"### Issue Resolution Statistics\"))\n",
        "\n",
        "                resolution_times = issue_timeline['resolution_times']\n",
        "\n",
        "                if resolution_times:\n",
        "                    # Calculate statistics\n",
        "                    avg_resolution = np.mean(resolution_times)\n",
        "                    median_resolution = np.median(resolution_times)\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <div style=\"background-color:#f5f5f5; padding:15px; border-radius:5px; margin:10px 0;\">\n",
        "                        <p><strong>Average Time to Close Issues:</strong> {avg_resolution:.2f} hours ({avg_resolution/24:.2f} days)</p>\n",
        "                        <p><strong>Median Time to Close Issues:</strong> {median_resolution:.2f} hours ({median_resolution/24:.2f} days)</p>\n",
        "                        <p><strong>Issues Analyzed:</strong> {len(resolution_times)}</p>\n",
        "                    </div>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    # Plot histogram of resolution times\n",
        "                    try:\n",
        "                        plt.figure(figsize=(10, 6))\n",
        "                        # Ensure all values are float and clip to reasonable range\n",
        "                        resolution_times_clean = np.array([float(rt) for rt in resolution_times if rt is not None])\n",
        "                        plt.hist(np.clip(resolution_times_clean, 0, 168), bins=20, alpha=0.7)  # Clip to one week for readability\n",
        "                        plt.title(\"Issue Resolution Times (Capped at 1 Week)\")\n",
        "                        plt.xlabel(\"Hours to Resolution\")\n",
        "                        plt.ylabel(\"Number of Issues\")\n",
        "                        plt.axvline(avg_resolution, color='r', linestyle='dashed', linewidth=1, label=f\"Mean: {avg_resolution:.2f} hours\")\n",
        "                        plt.axvline(median_resolution, color='g', linestyle='dashed', linewidth=1, label=f\"Median: {median_resolution:.2f} hours\")\n",
        "                        plt.legend()\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error generating issue resolution histogram: {str(e)}\")\n",
        "                        print(\"Skipping histogram visualization due to data compatibility issues.\")\n",
        "\n",
        "                # Display issue labels analysis\n",
        "                if issue_timeline.get('labels'):\n",
        "                    top_labels = sorted(issue_timeline['labels'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "                    if top_labels:\n",
        "                        display(Markdown(\"### Top Issue Labels\"))\n",
        "\n",
        "                        labels = [label for label, _ in top_labels]\n",
        "                        counts = [count for _, count in top_labels]\n",
        "\n",
        "                        try:\n",
        "                            plt.figure(figsize=(10, 6))\n",
        "\n",
        "                            # Limit label length for display and handle potential non-string labels\n",
        "                            cleaned_labels = []\n",
        "                            for label in labels:\n",
        "                                if isinstance(label, str):\n",
        "                                    # Truncate long labels\n",
        "                                    if len(label) > 20:\n",
        "                                        cleaned_labels.append(label[:17] + \"...\")\n",
        "                                    else:\n",
        "                                        cleaned_labels.append(label)\n",
        "                                else:\n",
        "                                    # Convert non-string labels to string\n",
        "                                    cleaned_labels.append(str(label))\n",
        "\n",
        "                            plt.bar(cleaned_labels, counts, alpha=0.7)\n",
        "                            plt.title(\"Most Common Issue Labels\")\n",
        "                            plt.xlabel(\"Label\")\n",
        "                            plt.ylabel(\"Count\")\n",
        "                            plt.xticks(rotation=45, ha='right')\n",
        "                            plt.tight_layout()\n",
        "                            plt.show()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error generating issue labels chart: {str(e)}\")\n",
        "                            print(\"Skipping labels visualization due to data compatibility issues.\")\n",
        "\n",
        "            # Display PR statistics\n",
        "            pr_timeline = repo_data[\"temporal_analysis\"][\"pr_timeline\"]\n",
        "            if pr_timeline:\n",
        "                display(Markdown(\"### Pull Request Statistics\"))\n",
        "\n",
        "                # Display PR acceptance rate\n",
        "                acceptance_rate = pr_timeline.get('acceptance_rate', 0)\n",
        "\n",
        "                display(HTML(f\"\"\"\n",
        "    <div style=\"background-color:#2c2c2c; color:#f5f5f5; padding:15px; border-radius:8px; margin:10px 0;\">\n",
        "        <p><strong>PR Acceptance Rate:</strong> {acceptance_rate:.2f}%</p>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "                # Display PR merge time statistics\n",
        "                if pr_timeline.get('merge_times'):\n",
        "                    merge_times = pr_timeline['merge_times']\n",
        "\n",
        "                    if merge_times:\n",
        "                        avg_merge = np.mean(merge_times)\n",
        "                        median_merge = np.median(merge_times)\n",
        "\n",
        "                        display(HTML(f\"\"\"\n",
        "                        <div style=\"background-color:#2c2c2c; color:#f5f5f5; padding:15px; border-radius:8px; margin:10px 0;\">\n",
        "                            <p><strong>Average Time to Merge PRs:</strong> {avg_merge:.2f} hours ({avg_merge/24:.2f} days)</p>\n",
        "                            <p><strong>Median Time to Merge PRs:</strong> {median_merge:.2f} hours ({median_merge/24:.2f} days)</p>\n",
        "                            <p><strong>PRs Analyzed:</strong> {len(merge_times)}</p>\n",
        "                        </div>\n",
        "                        \"\"\"))\n",
        "\n",
        "                        # Plot histogram of merge times\n",
        "                        try:\n",
        "                            plt.figure(figsize=(10, 6))\n",
        "                            # Ensure all values are float and clip to reasonable range\n",
        "                            merge_times_clean = np.array([float(mt) for mt in merge_times if mt is not None])\n",
        "                            plt.hist(np.clip(merge_times_clean, 0, 168), bins=20, alpha=0.7)  # Clip to one week for readability\n",
        "                            plt.title(\"PR Merge Times (Capped at 1 Week)\")\n",
        "                            plt.xlabel(\"Hours to Merge\")\n",
        "                            plt.ylabel(\"Number of PRs\")\n",
        "                            plt.axvline(avg_merge, color='r', linestyle='dashed', linewidth=1, label=f\"Mean: {avg_merge:.2f} hours\")\n",
        "                            plt.axvline(median_merge, color='g', linestyle='dashed', linewidth=1, label=f\"Median: {median_merge:.2f} hours\")\n",
        "                            plt.legend()\n",
        "                            plt.tight_layout()\n",
        "                            plt.show()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error generating PR merge time histogram: {str(e)}\")\n",
        "                            print(\"Skipping histogram visualization due to data compatibility issues.\")\n",
        "\n",
        "    def display_code_files(self, repo_data, max_files=5):\n",
        "        \"\"\"Display code files with syntax highlighting and complexity metrics.\"\"\"\n",
        "        if not repo_data or not repo_data[\"text_content\"] or not repo_data[\"text_content\"][\"text_files\"]:\n",
        "            return\n",
        "\n",
        "        display(Markdown(\"## Code File Preview\"))\n",
        "\n",
        "        # Filter for Python/JavaScript/TypeScript files\n",
        "        code_files = [\n",
        "            file for file in repo_data[\"text_content\"][\"text_files\"]\n",
        "            if file[\"name\"].endswith(('.py', '.js', '.ts', '.jsx', '.tsx'))\n",
        "        ]\n",
        "\n",
        "        # Sort by complexity if available\n",
        "        complexity_metrics = repo_data[\"text_content\"][\"complexity_metrics\"][\"cyclomatic_complexity\"]\n",
        "        complexity_dict = {path: cc for path, cc in complexity_metrics}\n",
        "\n",
        "        # Sort files by complexity (if available) or by file size\n",
        "        if complexity_dict:\n",
        "            code_files.sort(key=lambda x: complexity_dict.get(x[\"path\"], 0), reverse=True)\n",
        "        else:\n",
        "            code_files.sort(key=lambda x: len(x[\"content\"]), reverse=True)\n",
        "\n",
        "        # Display up to max_files\n",
        "        for i, file in enumerate(code_files[:max_files]):\n",
        "            file_path = file[\"path\"]\n",
        "            complexity = complexity_dict.get(file_path, \"N/A\")\n",
        "\n",
        "            display(Markdown(f\"### {file_path} (Complexity: {complexity})\"))\n",
        "\n",
        "            # Get code summary\n",
        "            summary = repo_data[\"text_content\"][\"code_summary\"].get(file_path, {})\n",
        "\n",
        "            # Display functions and classes\n",
        "            if summary.get(\"functions\") or summary.get(\"classes\"):\n",
        "                func_list = \", \".join(summary.get(\"functions\", []))\n",
        "                class_list = \", \".join(summary.get(\"classes\", []))\n",
        "\n",
        "                display(HTML(f\"\"\"\n",
        "                <div style=\"background-color:#2c2c2c; color:#f5f5f5; padding:10px; border-radius:5px; margin:5px 0; font-size:0.9em;\">\n",
        "                    <p><strong>Functions:</strong> {func_list or \"None\"}</p>\n",
        "                    <p><strong>Classes:</strong> {class_list or \"None\"}</p>\n",
        "                </div>\n",
        "                \"\"\"))\n",
        "\n",
        "            # Get file extension for syntax highlighting\n",
        "            ext = os.path.splitext(file[\"name\"])[1][1:]  # Remove the dot\n",
        "\n",
        "            # Display code with syntax highlighting (first 100 lines max)\n",
        "            code = file[\"content\"]\n",
        "            lines = code.split(\"\\n\")\n",
        "            preview_lines = lines[:min(100, len(lines))]\n",
        "            preview = \"\\n\".join(preview_lines)\n",
        "\n",
        "            display(Markdown(f\"```{ext}\\n{preview}\\n```\"))\n",
        "\n",
        "            if len(lines) > 100:\n",
        "                display(Markdown(f\"*... ({len(lines) - 100} more lines) ...*\"))\n",
        "\n",
        "    def export_repo_text(self, repo_data, output_dir='/content/repo_text'):\n",
        "        \"\"\"Export repository text content and analysis to files in Colab.\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Write README\n",
        "        if repo_data[\"text_content\"][\"readme\"] and repo_data[\"text_content\"][\"readme\"].get(\"content\"):\n",
        "            readme_path = os.path.join(output_dir, \"README.md\")\n",
        "            with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(repo_data[\"text_content\"][\"readme\"][\"content\"])\n",
        "\n",
        "        # Write documentation files\n",
        "        if repo_data[\"text_content\"][\"documentation\"]:\n",
        "            docs_dir = os.path.join(output_dir, \"docs\")\n",
        "            if not os.path.exists(docs_dir):\n",
        "                os.makedirs(docs_dir)\n",
        "\n",
        "            for doc in repo_data[\"text_content\"][\"documentation\"]:\n",
        "                # Create directory structure if needed\n",
        "                doc_path = os.path.join(docs_dir, doc[\"name\"])\n",
        "                with open(doc_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(doc[\"content\"])\n",
        "\n",
        "        # Write code files\n",
        "        code_dir = os.path.join(output_dir, \"code\")\n",
        "        if not os.path.exists(code_dir):\n",
        "            os.makedirs(code_dir)\n",
        "\n",
        "        for file in repo_data[\"text_content\"][\"text_files\"]:\n",
        "            if os.path.splitext(file[\"name\"])[1].lower() in ['.py', '.js', '.ts', '.jsx', '.tsx']:\n",
        "                file_path = os.path.join(code_dir, file[\"name\"])\n",
        "                with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(file[\"content\"])\n",
        "\n",
        "        # Write enhanced repository summary\n",
        "        summary_path = os.path.join(output_dir, \"repo_summary.md\")\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            # Get basic info\n",
        "            basic = repo_data[\"basic_info\"]\n",
        "\n",
        "            f.write(f\"# Repository Summary: {basic['full_name']}\\n\\n\")\n",
        "            f.write(f\"**Description:** {basic['description'] or 'No description'}\\n\\n\")\n",
        "            f.write(f\"**URL:** {basic['html_url']}\\n\")\n",
        "            f.write(f\"**Created:** {basic['created_at']}\\n\")\n",
        "            f.write(f\"**Last updated:** {basic['updated_at']}\\n\")\n",
        "            f.write(f\"**Default branch:** {basic['default_branch']}\\n\")\n",
        "            f.write(f\"**Stars:** {basic['stargazers_count']}\\n\")\n",
        "            f.write(f\"**Forks:** {basic['forks_count']}\\n\")\n",
        "            f.write(f\"**Open issues:** {basic['open_issues_count']}\\n\\n\")\n",
        "\n",
        "            # Analysis timestamp\n",
        "            f.write(f\"*Analysis performed: {repo_data['timestamp']}*\\n\\n\")\n",
        "\n",
        "            # Languages\n",
        "            if repo_data[\"languages\"]:\n",
        "                f.write(\"## Languages\\n\\n\")\n",
        "                total = sum(repo_data[\"languages\"].values())\n",
        "                for lang, bytes_count in repo_data[\"languages\"].items():\n",
        "                    percentage = (bytes_count / total) * 100\n",
        "                    f.write(f\"- **{lang}**: {percentage:.1f}% ({bytes_count} bytes)\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            # Contributors\n",
        "            if repo_data[\"contributors\"]:\n",
        "                f.write(\"## Top Contributors\\n\\n\")\n",
        "                for i, contributor in enumerate(repo_data[\"contributors\"][:10], 1):\n",
        "                    f.write(f\"{i}. {contributor['login']} - {contributor['contributions']} contributions\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            # Repository Activity\n",
        "            if repo_data[\"temporal_analysis\"][\"weekly_commits\"]:\n",
        "                f.write(\"## Repository Activity\\n\\n\")\n",
        "\n",
        "                # Recent commit activity\n",
        "                recent_weeks = repo_data[\"temporal_analysis\"][\"weekly_commits\"][-10:]\n",
        "                f.write(\"### Recent Commit Activity\\n\\n\")\n",
        "                f.write(\"| Week | Commits |\\n\")\n",
        "                f.write(\"|------|--------|\\n\")\n",
        "                for week in recent_weeks:\n",
        "                    f.write(f\"| {week['date']} | {week['total']} |\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "                # Issue and PR stats\n",
        "                issue_timeline = repo_data[\"temporal_analysis\"][\"issue_timeline\"]\n",
        "                pr_timeline = repo_data[\"temporal_analysis\"][\"pr_timeline\"]\n",
        "\n",
        "                if issue_timeline and issue_timeline.get('resolution_times'):\n",
        "                    avg_resolution = np.mean(issue_timeline['resolution_times'])\n",
        "                    median_resolution = np.median(issue_timeline['resolution_times'])\n",
        "\n",
        "                    f.write(\"### Issue Statistics\\n\\n\")\n",
        "                    f.write(f\"- Average time to close issues: {avg_resolution:.2f} hours ({avg_resolution/24:.2f} days)\\n\")\n",
        "                    f.write(f\"- Median time to close issues: {median_resolution:.2f} hours ({median_resolution/24:.2f} days)\\n\")\n",
        "                    f.write(f\"- Issues analyzed: {len(issue_timeline['resolution_times'])}\\n\\n\")\n",
        "\n",
        "                if pr_timeline and pr_timeline.get('merge_times'):\n",
        "                    avg_merge = np.mean(pr_timeline['merge_times'])\n",
        "                    median_merge = np.median(pr_timeline['merge_times'])\n",
        "\n",
        "                    f.write(\"### Pull Request Statistics\\n\\n\")\n",
        "                    f.write(f\"- PR acceptance rate: {pr_timeline['acceptance_rate']:.2f}%\\n\")\n",
        "                    f.write(f\"- Average time to merge PRs: {avg_merge:.2f} hours ({avg_merge/24:.2f} days)\\n\")\n",
        "                    f.write(f\"- Median time to merge PRs: {median_merge:.2f} hours ({median_merge/24:.2f} days)\\n\")\n",
        "                    f.write(f\"- PRs analyzed: {len(pr_timeline['merge_times'])}\\n\\n\")\n",
        "\n",
        "            # Code Complexity\n",
        "            if repo_data[\"text_content\"][\"complexity_metrics\"][\"cyclomatic_complexity\"]:\n",
        "                f.write(\"## Code Complexity\\n\\n\")\n",
        "\n",
        "                complexity_data = repo_data[\"text_content\"][\"complexity_metrics\"][\"cyclomatic_complexity\"]\n",
        "                complexity_data.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                f.write(\"### Most Complex Files\\n\\n\")\n",
        "                f.write(\"| File | Cyclomatic Complexity |\\n\")\n",
        "                f.write(\"|------|------------------------|\\n\")\n",
        "\n",
        "                for path, cc in complexity_data[:10]:\n",
        "                    f.write(f\"| {path} | {cc} |\\n\")\n",
        "\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "                # Get aggregate metrics\n",
        "                cc_values = [cc for _, cc in complexity_data]\n",
        "                f.write(f\"- **Average complexity**: {np.mean(cc_values):.2f}\\n\")\n",
        "                f.write(f\"- **Median complexity**: {np.median(cc_values):.2f}\\n\")\n",
        "                f.write(f\"- **Max complexity**: {np.max(cc_values)}\\n\")\n",
        "                f.write(f\"- **Files analyzed**: {len(cc_values)}\\n\\n\")\n",
        "\n",
        "            # Code Dependencies\n",
        "            if repo_data[\"text_content\"][\"dependencies\"]:\n",
        "                f.write(\"## Code Dependencies\\n\\n\")\n",
        "\n",
        "                external_deps = repo_data[\"text_content\"][\"dependencies\"][\"external\"]\n",
        "\n",
        "                # Count unique external dependencies\n",
        "                all_external = set()\n",
        "                for deps in external_deps.values():\n",
        "                    all_external.update(deps)\n",
        "\n",
        "                # Find most imported packages\n",
        "                ext_counts = Counter()\n",
        "                for deps in external_deps.values():\n",
        "                    ext_counts.update(deps)\n",
        "\n",
        "                top_imports = ext_counts.most_common(10)\n",
        "\n",
        "                f.write(\"### Most Used External Dependencies\\n\\n\")\n",
        "                f.write(\"| Package | Used in # Files |\\n\")\n",
        "                f.write(\"|---------|----------------|\\n\")\n",
        "\n",
        "                for pkg, count in top_imports:\n",
        "                    f.write(f\"| {pkg} | {count} |\\n\")\n",
        "\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            # Code Summary\n",
        "            if repo_data[\"text_content\"][\"code_summary\"]:\n",
        "                f.write(\"## Code Structure\\n\\n\")\n",
        "\n",
        "                # Get summary of most significant files\n",
        "                complexity_data = repo_data[\"text_content\"][\"complexity_metrics\"][\"cyclomatic_complexity\"]\n",
        "                complexity_data.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                for path, _ in complexity_data[:5]:\n",
        "                    summary = repo_data[\"text_content\"][\"code_summary\"].get(path)\n",
        "                    if summary:\n",
        "                        f.write(f\"### {path}\\n\\n\")\n",
        "\n",
        "                        if summary.get(\"description\"):\n",
        "                            f.write(f\"{summary['description']}\\n\\n\")\n",
        "\n",
        "                        if summary.get(\"classes\"):\n",
        "                            f.write(\"**Classes:**\\n\\n\")\n",
        "                            for cls in summary[\"classes\"]:\n",
        "                                f.write(f\"- `{cls}`\\n\")\n",
        "                            f.write(\"\\n\")\n",
        "\n",
        "                        if summary.get(\"functions\"):\n",
        "                            f.write(\"**Functions:**\\n\\n\")\n",
        "                            for func in summary[\"functions\"]:\n",
        "                                f.write(f\"- `{func}()`\\n\")\n",
        "                            f.write(\"\\n\")\n",
        "\n",
        "                        if summary.get(\"imports\"):\n",
        "                            f.write(\"**Imports:**\\n\\n\")\n",
        "                            for imp in summary[\"imports\"][:10]:  # Limit to top 10\n",
        "                                if isinstance(imp, tuple):\n",
        "                                    imp = ' '.join(filter(None, imp))\n",
        "                                f.write(f\"- `{imp}`\\n\")\n",
        "                            f.write(\"\\n\")\n",
        "\n",
        "        # Write visualization exports\n",
        "        viz_dir = os.path.join(output_dir, \"visualizations\")\n",
        "        if not os.path.exists(viz_dir):\n",
        "            os.makedirs(viz_dir)\n",
        "\n",
        "        # Export language distribution chart\n",
        "        if repo_data[\"languages\"]:\n",
        "            lang_data = []\n",
        "            total = sum(repo_data[\"languages\"].values())\n",
        "            for lang, bytes_count in repo_data[\"languages\"].items():\n",
        "                percentage = (bytes_count / total) * 100\n",
        "                lang_data.append({\n",
        "                    \"Language\": lang,\n",
        "                    \"Percentage\": percentage\n",
        "                })\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.pie([d[\"Percentage\"] for d in lang_data],\n",
        "                   labels=[d[\"Language\"] for d in lang_data],\n",
        "                   autopct='%1.1f%%')\n",
        "            plt.title(\"Language Distribution\")\n",
        "            plt.axis('equal')\n",
        "            plt.savefig(os.path.join(viz_dir, \"language_distribution.png\"), bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        # Export commit activity timeline\n",
        "        if repo_data[\"temporal_analysis\"][\"weekly_commits\"]:\n",
        "            weekly_commits = repo_data[\"temporal_analysis\"][\"weekly_commits\"]\n",
        "\n",
        "            dates = [datetime.strptime(week['date'], '%Y-%m-%d') for week in weekly_commits]\n",
        "            commits = [week['total'] for week in weekly_commits]\n",
        "\n",
        "            plt.figure(figsize=(14, 6))\n",
        "            plt.plot(dates, commits, marker='o', linestyle='-', alpha=0.7)\n",
        "            plt.title(\"Weekly Commit Activity\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Number of Commits\")\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Format x-axis to show dates nicely\n",
        "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "            plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
        "            plt.gcf().autofmt_xdate()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(viz_dir, \"commit_activity.png\"), bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        print(f\"\\nRepository analysis exported to {output_dir}\")\n",
        "\n",
        "        # Provide option to download files\n",
        "        print(\"\\nTo download exported files, run the following cell:\")\n",
        "        print(\"from google.colab import files\")\n",
        "        print(\"files.download('/content/repo_text/repo_summary.md')\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "# Colab-Specific Helper Functions\n",
        "def download_file(file_path):\n",
        "    \"\"\"Download a file from Colab.\"\"\"\n",
        "    files.download(file_path)\n",
        "\n",
        "def save_json_to_colab(data, filename='/content/repo_info.json'):\n",
        "    \"\"\"Save JSON data to a file in Colab and provide download option.\"\"\"\n",
        "    # Create a custom encoder that can handle sets and other non-serializable types\n",
        "    class CustomJSONEncoder(json.JSONEncoder):\n",
        "        def default(self, obj):\n",
        "            if isinstance(obj, set):\n",
        "                return list(obj)\n",
        "            elif isinstance(obj, datetime):\n",
        "                return obj.isoformat()\n",
        "            elif hasattr(obj, '__dict__'):\n",
        "                return obj.__dict__\n",
        "            return super(CustomJSONEncoder, self).default(obj)\n",
        "\n",
        "    # Create a recursive function to convert sets to lists in the data structure\n",
        "    def convert_sets_to_lists(obj):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: convert_sets_to_lists(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_sets_to_lists(i) for i in obj]\n",
        "        elif isinstance(obj, set):\n",
        "            return [convert_sets_to_lists(i) for i in obj]\n",
        "        elif isinstance(obj, tuple):\n",
        "            return tuple(convert_sets_to_lists(i) for i in obj)\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    # Convert sets to lists in the data\n",
        "    converted_data = convert_sets_to_lists(data)\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(converted_data, f, indent=2, cls=CustomJSONEncoder)\n",
        "\n",
        "    print(f\"Data saved to {filename}\")\n",
        "    print(\"To download the JSON file, run the following cell:\")\n",
        "    print(f\"from google.colab import files\")\n",
        "    print(f\"files.download('{filename}')\")\n",
        "\n",
        "\n",
        "# Main function for running in Colab\n",
        "def run_github_analyzer():\n",
        "    \"\"\"Run the GitHub repository analyzer in Colab.\"\"\"\n",
        "    # Get user input\n",
        "    owner = input(\"Enter the repository owner (username or organization): \")\n",
        "    repo = input(\"Enter the repository name: \")\n",
        "    token = input(\"Enter your GitHub token (optional but recommended to avoid rate limits): \")\n",
        "\n",
        "    # Initialize the analyzer\n",
        "    analyzer = GitHubRepoInfo(token=token if token else None)\n",
        "\n",
        "    # Display notices about rate limits and analysis time\n",
        "    print(\"\\nNOTE: Comprehensive analysis may take several minutes depending on repository size.\")\n",
        "    print(\"Using a GitHub token is recommended to avoid API rate limits.\")\n",
        "    print(\"Analysis will be performed with reasonable limits to prevent excessive API calls.\")\n",
        "\n",
        "    # Get repository information\n",
        "    print(f\"\\nFetching repository information for {owner}/{repo}...\")\n",
        "    repo_data = analyzer.get_all_info(owner, repo)\n",
        "\n",
        "    if repo_data:\n",
        "        # Display repository information\n",
        "        print(\"\\nGenerating visualizations and analysis...\")\n",
        "        analyzer.display_repo_info(repo_data)\n",
        "\n",
        "        # Display code files preview\n",
        "        analyzer.display_code_files(repo_data)\n",
        "\n",
        "        # Save data to JSON\n",
        "        save_to_json = input(\"\\nSave repository data to JSON? (y/n): \").lower() == 'y'\n",
        "        if save_to_json:\n",
        "            filename = input(\"Enter filename (default: /content/repo_info.json): \") or '/content/repo_info.json'\n",
        "            save_json_to_colab(repo_data, filename)\n",
        "\n",
        "        # Export text content\n",
        "        export_text = input(\"\\nExport repository text content and analysis? (y/n): \").lower() == 'y'\n",
        "        if export_text:\n",
        "            output_dir = input(\"Enter output directory (default: /content/repo_text): \") or '/content/repo_text'\n",
        "            analyzer.export_repo_text(repo_data, output_dir)\n",
        "    else:\n",
        "        print(f\"Failed to get repository information for {owner}/{repo}\")\n",
        "\n",
        "# Interactive cell for running the analyzer\n",
        "# To use, run this cell and follow the prompts\n",
        "print(\"Enhanced GitHub Repository Information Tool for Google Colab\")\n",
        "print(\"=\"*65)\n",
        "print(\"\\nThis tool fetches comprehensive information about a GitHub repository\")\n",
        "print(\"and provides detailed analysis of code, activity, and structure.\")\n",
        "print(\"\\nFeatures:\")\n",
        "print(\"- Code complexity analysis\")\n",
        "print(\"- Dependency mapping\")\n",
        "print(\"- Temporal activity tracking\")\n",
        "print(\"- Issue and PR resolution metrics\")\n",
        "print(\"- Advanced code structure visualization\")\n",
        "print(\"\\nTo begin, run the 'run_github_analyzer()' function in the next cell.\")"
      ],
      "metadata": {
        "id": "8uZ5S1x_xwmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}